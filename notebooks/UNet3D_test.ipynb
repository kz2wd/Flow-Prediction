{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ffb9bd-0433-4859-a49c-4d66b7baae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:47:01.756373: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-17 14:47:01.768809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752731221.783602 1582842 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752731221.787833 1582842 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752731221.799533 1582842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752731221.799550 1582842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752731221.799551 1582842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752731221.799553 1582842 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-17 14:47:01.803699: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet3DConditionModel\n",
    "\n",
    "model = UNet3DConditionModel(\n",
    "    sample_size=(64, 64, 64),\n",
    "    in_channels=6,         # your actual input channels\n",
    "    out_channels=3,        # your desired output channels\n",
    "    block_out_channels=(64, 128, 256, 512),\n",
    "    layers_per_block=2,\n",
    "    down_block_types=(\"DownBlock3D\", \"DownBlock3D\", \"DownBlock3D\", \"DownBlock3D\"),\n",
    "    up_block_types=(\"UpBlock3D\", \"UpBlock3D\", \"UpBlock3D\", \"UpBlock3D\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc3cfc8-bc35-4be2-ab01-d5e76cb9e2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.00E+08'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%.2E\" % model.num_parameters(only_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b6a04e0-d7be-4079-9bef-a4a9217d91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import SchedulerMixin, ConfigMixin\n",
    "\n",
    "class SchedulerSubNoiseWrapper(SchedulerMixin, ConfigMixin):\n",
    "    def __init__(self, base_scheduler, noise_affected_channels):\n",
    "        self.base_scheduler = base_scheduler\n",
    "        self.noise_affected_channels = noise_affected_channels\n",
    "        self.intern_config = {\n",
    "            \"noise_affected_channels\": noise_affected_channels,\n",
    "            \"base_scheduler_config\": base_scheduler.config,\n",
    "            \"base_class\": base_scheduler.__class__.__name__\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        # Return your wrapped config\n",
    "        return self.base_scheduler.config\n",
    "    \n",
    "    def add_noise(self, sample, noise, timesteps):\n",
    "        # Sample combinaison of ( Y of shape (B, out_channels), and extra info )\n",
    "        # sample of shape (B, in_channels)\n",
    "        # noise is expected to be of shape (B, out_channels)\n",
    "        noised_channels = self.base_scheduler.add_noise(sample[:, :self.noise_affected_channels], noise, timesteps)\n",
    "        return torch.cat([noised_channels, sample[:, self.noise_affected_channels:]], dim=1)\n",
    "\n",
    "    def step(self, model_output, timestep, sample, **kwargs):\n",
    "        noisy_part = sample[:, :self.noise_affected_channels]\n",
    "        informative_part = sample[:, self.noise_affected_channels:]\n",
    "\n",
    "        # Step only on noisy part\n",
    "        step_output = self.base_scheduler.step(model_output, timestep, noisy_part, **kwargs)\n",
    "        new_vel = step_output.prev_sample\n",
    "\n",
    "        # Recombine full input, dim=1 because of batching\n",
    "        full_sample = torch.cat([new_vel, informative_part], dim=1)\n",
    "\n",
    "        # Return modified StepOutput with recombined tensor\n",
    "        return type(step_output)(**{**step_output.__dict__, \"prev_sample\": full_sample})\n",
    "\n",
    "        \n",
    "    def save_config(self, **kwargs):\n",
    "        return self.base_scheduler.save_config(**kwargs)\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        import json\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        self.base_scheduler.save_pretrained(os.path.join(save_directory, \"base_scheduler\"))\n",
    "        with open(os.path.join(save_directory, \"wrapper_config.json\"), \"w\") as f:\n",
    "            json.dump(self.intern_config, f)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory, **kwargs):\n",
    "        import importlib\n",
    "        import json\n",
    "        from diffusers import SchedulerMixin\n",
    "\n",
    "        config_path = os.path.join(save_directory, \"wrapper_config.json\")\n",
    "        with open(config_path, \"r\") as f:\n",
    "            wd = json.load(f)\n",
    "        base_name = wd[\"base_class\"]\n",
    "        base_scheduler = SchedulerMixin.from_pretrained(os.path.join(save_directory, \"base_scheduler\"))\n",
    "        return cls(base_scheduler, wd[\"noise_affected_channels\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "877d0e18-b0c1-473a-803e-092b910fb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "base_scheduler = DDPMScheduler(num_train_timesteps=50)\n",
    "scheduler = SchedulerSubNoiseWrapper(base_scheduler, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "804c6cd8-19cf-4d1e-9575-895043236862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDPMScheduler {\n",
       "  \"_class_name\": \"DDPMScheduler\",\n",
       "  \"_diffusers_version\": \"0.34.0\",\n",
       "  \"beta_end\": 0.02,\n",
       "  \"beta_schedule\": \"linear\",\n",
       "  \"beta_start\": 0.0001,\n",
       "  \"clip_sample\": true,\n",
       "  \"clip_sample_range\": 1.0,\n",
       "  \"dynamic_thresholding_ratio\": 0.995,\n",
       "  \"num_train_timesteps\": 50,\n",
       "  \"prediction_type\": \"epsilon\",\n",
       "  \"rescale_betas_zero_snr\": false,\n",
       "  \"sample_max_value\": 1.0,\n",
       "  \"steps_offset\": 0,\n",
       "  \"thresholding\": false,\n",
       "  \"timestep_spacing\": \"leading\",\n",
       "  \"trained_betas\": null,\n",
       "  \"variance_type\": \"fixed_small\"\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.base_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f47ca22-aee7-4dca-a0ac-74f0b6f5fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    train_batch_size = 2\n",
    "    eval_batch_size = 1  # how many images to sample during evaluation\n",
    "    num_epochs = 5\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 5\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"UNet-flow-test\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n",
    "    hub_private_repo = None\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2d6d62-5c84-4751-a10b-852a2ca401dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading std & mean of dataset re200-sr05etot\n",
      "Loading stds & means of dataset re200-sr05etot\n",
      "âŒ› Initializing Dataset...\n",
      "X...\n",
      "[########################################] | 100% Completed | 344.78 ms\n",
      "Y...\n",
      "[########################################] | 100% Completed | 721.71 ms\n"
     ]
    }
   ],
   "source": [
    "# from space_exploration.beans.training_bean import Training\n",
    "from space_exploration.beans.dataset_bean import Dataset\n",
    "from space_exploration.training.training import ModelTraining\n",
    "from space_exploration.dataset.transforms.AllTransforms import TransformationReferences\n",
    "from space_exploration.training.training_utils import prepare_dataset\n",
    "dataset_name = \"re200-sr05etot\"\n",
    "dataset = Dataset.get_dataset_or_fail(dataset_name)\n",
    "ds = dataset.get_training_dataset(64, TransformationReferences.UNET_ADAPTER64.transformation, TransformationReferences.Y_ALONG_COMPONENT_NORMALIZE.transformation, \n",
    "                                  size=6)\n",
    "train_dataloader = prepare_dataset(ds, batch_size=config.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e73ab9be-777b-4857-86d4-319a73450e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1506a8229240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae9cbf8-d3a7-48b4-b659-567b1a90c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4396542-9d44-471a-bfbd-995a63a436cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.to(\"cuda\")\n",
    "sample_input, sample_output = next(iter(ds))\n",
    "sample_input = torch.tensor(np.array([sample_input]))\n",
    "print(\"Input shape:\", sample_input.shape)\n",
    "sample_input = sample_input.to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "417c2cc2-a46e-4a43-aa59-8562a7e4ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = torch.tensor(np.array([sample_output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16fe106e-86bd-4703-8649-9b2fcfc93c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    encoder_hidden_states = torch.ones((1, 1, 1024), device=model.device)\n",
    "    forward = model(sample_input, timestep=0, encoder_hidden_states=encoder_hidden_states)\n",
    "    print(\"Output shape:\", forward.sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc7988-491b-432f-be85-4116164f4d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f321299-cd4d-461a-aa67-9ff83c65ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "noise_pred = forward.sample.to(\"cpu\")\n",
    "loss = F.mse_loss(noise_pred, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b7f0ba-41dc-4e66-b88d-97e29773a2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3968)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b1213e7-0b78-4223-9ea1-1b6b319ca182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "from diffusers import DDPMPipeline\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        if config.push_to_hub:\n",
    "            repo_id = create_repo(\n",
    "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
    "            ).repo_id\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Define mock hidden states\n",
    "    encoder_hidden_states = torch.ones((config.train_batch_size, 1, 1024), device=model.device)\n",
    "    \n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "            bs = x.shape[0]\n",
    "            noise_shape = y.shape\n",
    "            noise = torch.randn(noise_shape, device=x.device)\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=x.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            y_with_extra_info = torch.cat([y, x[:, 3:]], dim=1)\n",
    "            noisy_input = noise_scheduler.add_noise(y_with_extra_info, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_input, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            # if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            #     evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    upload_folder(\n",
    "                        repo_id=repo_id,\n",
    "                        folder_path=config.output_dir,\n",
    "                        commit_message=f\"Epoch {epoch}\",\n",
    "                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "                    )\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1aca664a-759e-4a36-bd3c-f37da67a1a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203debadda8d45be82862dad0c269479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9032db07021e4d24b1888696dd18b003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eadaed3a02436d8a0b06c78533a1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abf80aad9d44007b1cfead5b6c77b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080086e9c57941efadd8362707e3d4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f428e3c-79f4-430e-a373-981a6e31d51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d5f68c8175472e896f28d1525a257b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/data3/chancrin/code/3D-GAN/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'SchedulerMixin' has no attribute 'load_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DiffusionPipeline\n\u001b[0;32m----> 3\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data3/chancrin/code/3D-GAN/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data3/chancrin/code/3D-GAN/.venv/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:1022\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m     sub_model_dtype \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1018\u001b[0m         torch_dtype\u001b[38;5;241m.\u001b[39mget(name, torch_dtype\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch_dtype\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[0;32m-> 1022\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_model_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdduf_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m     )\n\u001b[1;32m   1051\u001b[0m init_kwargs[name] \u001b[38;5;241m=\u001b[39m loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
      "File \u001b[0;32m/data3/chancrin/code/3D-GAN/.venv/lib/python3.10/site-packages/diffusers/pipelines/pipeline_loading_utils.py:830\u001b[0m, in \u001b[0;36mload_sub_model\u001b[0;34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder, use_safetensors, dduf_entries, provider_options, quantization_config)\u001b[0m\n\u001b[1;32m    828\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, name)):\n\u001b[0;32m--> 830\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloading_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# else load from the root directory\u001b[39;00m\n\u001b[1;32m    833\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(cached_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n",
      "Cell \u001b[0;32mIn[39], line 60\u001b[0m, in \u001b[0;36mSchedulerSubNoiseWrapper.from_pretrained\u001b[0;34m(cls, save_directory, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     wd \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     59\u001b[0m base_name \u001b[38;5;241m=\u001b[39m wd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_class\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m base_scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mSchedulerMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_scheduler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(base_scheduler, wd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnoise_affected_channels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/data3/chancrin/code/3D-GAN/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data3/chancrin/code/3D-GAN/.venv/lib/python3.10/site-packages/diffusers/schedulers/scheduling_utils.py:151\u001b[0m, in \u001b[0;36mSchedulerMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    103\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Instantiate a scheduler from a pre-defined JSON configuration file in a local directory or Hub repository.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     config, kwargs, commit_hash \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m(\n\u001b[1;32m    152\u001b[0m         pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path,\n\u001b[1;32m    153\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    154\u001b[0m         return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m         return_commit_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_config(config, return_unused_kwargs\u001b[38;5;241m=\u001b[39mreturn_unused_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'SchedulerMixin' has no attribute 'load_config'"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(config.output_dir, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24502d-14ee-4765-9b6f-eff9d22135d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
